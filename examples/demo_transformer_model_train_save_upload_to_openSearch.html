

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch &mdash; Opensearch-py-ml 1.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=6efca38a"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Demo Notebook to trace Sentence Transformers model" href="demo_tracing_model_torchscript_onnx.html" />
    <link rel="prev" title="Demo Notebook for Online Retail Analysis" href="online_retail_analysis.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Opensearch-py-ml
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#demo-notebooks-for-data-exploration-panda-like-dataframe">Demo notebooks for Data Exploration Panda like DataFrame</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#demo-notebooks-for-model-training-and-tracing">Demo notebooks for Model Training and Tracing</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Download-notebook">Download notebook</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="demo_tracing_model_torchscript_onnx.html">Demo Notebook to trace Sentence Transformers model</a></li>
<li class="toctree-l3"><a class="reference internal" href="demo_deploy_cliptextmodel.html">Demo Notebook for deploying CLIPTextModel to OpenSearch</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#demo-notebooks-for-ml-commons-plugin-integration">Demo notebooks for ML Commons plugin integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#demo-notebooks-for-in-house-python-based-models">Demo Notebooks for In-house python based models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">Command Line Interface (CLI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/pre_trained_models.html">Pre-trained models</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Opensearch-py-ml</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Examples</a></li>
      <li class="breadcrumb-item active">Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/demo_transformer_model_train_save_upload_to_openSearch.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Demo-Notebook-for-Sentence-Transformer-Model-Training,-Saving-and-Uploading-to-OpenSearch">
<h1>Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch<a class="headerlink" href="#Demo-Notebook-for-Sentence-Transformer-Model-Training,-Saving-and-Uploading-to-OpenSearch" title="Link to this heading"></a></h1>
<section id="Download-notebook">
<h2><a class="reference external" href="https://github.com/opensearch-project/opensearch-py-ml/blob/main/docs/source/examples/demo_transformer_model_train_save_upload_to_openSearch.ipynb">Download notebook</a><a class="headerlink" href="#Download-notebook" title="Link to this heading"></a></h2>
<section id="Introduction">
<h3>Introduction<a class="headerlink" href="#Introduction" title="Link to this heading"></a></h3>
<p>This notebook introduces the technique of synthetic data generation and how it can be used to obtain a deep learning model for Search that is custom built for a given set of documents.</p>
<p>Deep learning models are very powerful and have been shown to improve state of the art in several disciplines and tasks. However, they need a lot of labelled training data. Such data is often hard to obtain. In this notebook, we show how pre-trained large language models can be used to circumvent this issue.</p>
<p>We focus on the task of passage retrieval i.e the corpus consists of passages which is searched at run-time given a user query. This search can be performed by transformers such as BERT as long as BERT is trained on a labelled dataset that consists of pairs such as (queries, relevant passage). Such a BERT model can be used for semantic search.</p>
<section id="Synthetic-query-generation">
<h4>Synthetic query generation<a class="headerlink" href="#Synthetic-query-generation" title="Link to this heading"></a></h4>
<p>In the absence of such labelled data we provide a synthetic query generator (SQG) model that can be used to create synthetic queries given a passage. The SQG model is a large transformer model that has been trained to generate human like queries given a passage. Thus it can be used to create a labelled dataset of (synthetic queries, passage). A BERT model can be trained on this synthetic data and used for semantic search. In fact, we find that such synthetically trained models beat the current
state-of-the-art models. Note that resulting BERT model is a customized model since it has been trained on a specific corpus (and corresponding synthetic queries).</p>
<p>This notebook provides an end-to-end guide for users to generate synthetic queries and fine-tune a sentence transformer model on it using opensearch_py_ml. It consists of the following steps,</p>
<p>Step 1: Import packages and set up client</p>
<p>Step 2: Import the data/passages for synthetic query generation</p>
<p>Step 3: Generate Synthetic Queries</p>
<p>Step 4: Read synthetic queries and train/fine-tune model using a hugging face sentence transformer model</p>
<p>Step 5: Upload the model to OpenSearch cluster</p>
<p>Steps 3 and 4 are compute intensive step, and we recommend running it on a machine with 4 or more GPUS such as the EC2 <code class="docutils literal notranslate"><span class="pre">p3.8xlarge</span></code> or <code class="docutils literal notranslate"><span class="pre">p3.16xlarge</span></code>.</p>
</section>
</section>
<section id="Step-1:-Import-packages,-set-up-client-and-define-helper-functions">
<h3>Step 1: Import packages, set up client and define helper functions<a class="headerlink" href="#Step-1:-Import-packages,-set-up-client-and-define-helper-functions" title="Link to this heading"></a></h3>
<p>Install required packages for opensearch_py_ml.sentence_transformer_model Install <code class="docutils literal notranslate"><span class="pre">opensearchpy</span></code> and <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> through pypi</p>
<p>generate.py script is released with the Synthetic Query Generation model.</p>
<p>Please refer <a class="reference external" href="https://pytorch.org/">https://pytorch.org/</a> to proper install torch based on your environment setting.</p>
<p>Please install the following packages from the terminal if you haven’t already. They can be also installed from the notebook by uncommenting the line and execute.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download generate.py for Generate Synthetic Queries</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s2">&quot;https://artifacts.opensearch.org/models/ml-models/amazon/gpt/GPT2_xl_sqg/1.0.0/generate.py&quot;</span><span class="p">,</span> <span class="s2">&quot;generate.py&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(&#39;generate.py&#39;, &lt;http.client.HTTPMessage at 0x10bdbe940&gt;)
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !pip install pandas matplotlib numpy torch accelerate sentence_transformers tqdm transformers opensearch-py opensearch-py-ml detoxify datasets</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Unverified HTTPS request&quot;</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">opensearch_py_ml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">oml</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opensearchpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenSearch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">generate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">generate</span><span class="w"> </span><span class="kn">import</span> <span class="n">Synthetic_Query_Generation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opensearch_py_ml.ml_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformerModel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">boto3</span><span class="o">,</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span><span class="o">,</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import mlcommon to later upload the model to OpenSearch Cluster</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opensearch_py_ml.ml_commons</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLCommonClient</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">CLUSTER_URL</span> <span class="o">=</span> <span class="s1">&#39;https://localhost:9200&#39;</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_os_client</span><span class="p">(</span><span class="n">cluster_url</span> <span class="o">=</span> <span class="n">CLUSTER_URL</span><span class="p">,</span>
                  <span class="n">username</span><span class="o">=</span><span class="s1">&#39;admin&#39;</span><span class="p">,</span>
                  <span class="n">password</span><span class="o">=</span><span class="s1">&#39;&lt; admin password &gt;&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Get OpenSearch client</span>
<span class="sd">    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443</span>
<span class="sd">    :return: OpenSearch client</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenSearch</span><span class="p">(</span>
        <span class="n">hosts</span><span class="o">=</span><span class="p">[</span><span class="n">cluster_url</span><span class="p">],</span>
        <span class="n">http_auth</span><span class="o">=</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">),</span>
        <span class="n">verify_certs</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">client</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">get_os_client</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">myselect</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;passages&quot;</span><span class="p">][</span><span class="s2">&quot;is_selected&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;passages&quot;</span><span class="p">][</span><span class="s2">&quot;passage_text&quot;</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;passages&quot;</span><span class="p">][</span><span class="s2">&quot;is_selected&quot;</span><span class="p">])]</span>
    <span class="k">return</span> <span class="s2">&quot;-1&quot;</span>
</pre></div>
</div>
</div>
</section>
<section id="Step-2:-Import-the-data/passages-for-synthetic-query-generation">
<h3>Step 2: Import the data/passages for synthetic query generation<a class="headerlink" href="#Step-2:-Import-the-data/passages-for-synthetic-query-generation" title="Link to this heading"></a></h3>
<p>There are three supported options to read datasets :</p>
<ul class="simple">
<li><p>Option 1: read from a local data folder in jsonl file</p></li>
<li><p>Option 2: read from a list of passages</p></li>
<li><p>Option 3: read from OpenSearch client by index_name</p></li>
</ul>
<p>For the purpose of this notebook we will demonstrate option 2: read from a list of passages.</p>
<p>We take the MS Marco dataset of passages as our example dataset.</p>
<section id="2.1)-Load-the-data-and-convert-into-a-pandas-dataframe">
<h4>2.1) Load the data and convert into a pandas dataframe<a class="headerlink" href="#2.1)-Load-the-data-and-convert-into-a-pandas-dataframe" title="Link to this heading"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;ms_marco&quot;</span><span class="p">,</span><span class="s2">&quot;v1.1&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;passage&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">myselect</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span><span class="s2">&quot;passage&quot;</span><span class="p">]][</span><span class="n">df</span><span class="o">.</span><span class="n">passage</span> <span class="o">!=</span> <span class="s2">&quot;-1&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setting print options to display full columns</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.expand_frame_repr&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;max_colwidth&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The dataset looks like,</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>The MS Marco dataset has real queries for passages but we will pretend that it does not and generate synthetic queries for each passage</p>
</section>
<section id="2.2)-Convert-the-data-into-a-list-of-strings-and-instantiate-an-object-of-the-class-Synthetic_Query_Generation">
<h4>2.2) Convert the data into a list of strings and instantiate an object of the class Synthetic_Query_Generation<a class="headerlink" href="#2.2)-Convert-the-data-into-a-list-of-strings-and-instantiate-an-object-of-the-class-Synthetic_Query_Generation" title="Link to this heading"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_passages</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">passage</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ss</span> <span class="o">=</span> <span class="n">Synthetic_Query_Generation</span><span class="p">(</span><span class="n">sentences</span> <span class="o">=</span> <span class="n">sample_passages</span><span class="p">[:</span><span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="Step-3:-Generate-synthetic-queries">
<h3>Step 3: Generate synthetic queries<a class="headerlink" href="#Step-3:-Generate-synthetic-queries" title="Link to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">three_step_query</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">generate_synthetic_queries</span><span class="p">(</span><span class="n">num_machines</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                                                 <span class="n">tokenize_data</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                                 <span class="n">tokenizer_max_length</span> <span class="o">=</span>  <span class="mi">300</span><span class="p">,</span>
                                                 <span class="n">total_queries</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                                                 <span class="n">numseq</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                                                 <span class="n">num_gpu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                                                 <span class="n">toxic_cutoff</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
                                                 <span class="n">tokens_to_word_ratio</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A lot of actions are being executed in the above cell. We elaborate them step by step,</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1) Convert the data into a form that can be consumed by the Synthetic query generator (SQG) model. This amounts to tokenizing the data using a tokenizer. The SQG model is a fine-tuned version of the GPT-XL model https://huggingface.co/gpt2-xl and the tokenizer is the GPT tokenizer.

2) The tokenizer has a max input length of 512 tokens. Every passage is tokenized with the special tokens &lt;|startoftext|&gt; and QRY: appended to the beginning and the end of every passage respectively. Note that tokenization is a time intensive process and the script saves the tokenized data after the first pass. We recommend setting tokenize_data = False subsequently.

3) Load the SQG model i.e. 1.5B parameter GPT2-XL model that has been trained to ask questions given passages. This model has been made publicly available and can be found here: https://artifacts.opensearch.org/models/ml-models/amazon/gpt/GPT2_xl_sqg/1.0.0/GPT2_xl_sqg.zip

4) Once the model has been loaded and the data has been tokenized, the model starts the process of query generation. &quot;total_queries&quot; is number of synthetic queries generated for every passage and &quot;numseq&quot; is the number of queries that are generated by a model at a given time. Ideally total_queries = numseq, but this can lead to out of memory issues. So set numseq to an integer that is around 10 or less, and is a divisor of total_queries.

5) tokens_to_word_ratio is a float variable that is used to switch between length of a document in tokens vs. in words. It is used when truncating documents during the tokenization phase. Most words are split in to one or more tokens. A document that has a length of 300 tokens might only be 200 words long. This ratio of 200/300 = 2/3 = 0.667 is the tokens_to_word_ratio. For passages from a dataset such as Wikipedia this ratio is around 0.65 to 0.7, but for domain specific datasets this ratio could be as small as 0.5.

6) The script also requires to know the number of GPUs and the number of machines/nodes that it can use. Since we are using a single node instance with no GPUs we pass 0 and 1 to the function respectively. Our recommended setting is to use 1 machine/node with at least 4 (ideally 8) GPUs.

7) The script now begins to generate queries and displays a progress bar. We create total_queries per passage. Empirically we find that generating more queries leads to better performance but there are diminishing returns since the total inference time increases with total_queries.

8) After generating the queries, the function uses a publicly available package called Detoxify to remove inappropriate queries from the dataset. &quot;toxic_cutoff&quot; is a float. The script rejects all queries that have a toxicity score greater than toxic_cutoff

9) Finally, the synthetic queries along with their corresponding passages are saved in a zipped file in the current working directory.
</pre></div>
</div>
<p>Note – Please restart the kernel and rerun it if the notebook gives CUDA related errors.</p>
<section id="This-is-how-the-sample-queries-look-like,">
<h4>This is how the sample queries look like,<a class="headerlink" href="#This-is-how-the-sample-queries-look-like," title="Link to this heading"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># initiate SentenceTransformerModel object</span>

<span class="n">custom_model</span> <span class="o">=</span> <span class="n">SentenceTransformerModel</span><span class="p">(</span><span class="n">folder_path</span><span class="o">=</span><span class="s2">&quot;/Volumes/workplace/upload_content/model_files/&quot;</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>



<span class="n">df</span> <span class="o">=</span> <span class="n">custom_model</span><span class="o">.</span><span class="n">read_queries</span><span class="p">(</span><span class="n">read_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/clean_synthetic_queries.zip&#39;</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[::</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="Step-4:-Read-synthetic-queries-and-train/fine-tune-a-hugging-face-sentence-transformer-model-on-synthetic-data">
<h3>Step 4: Read synthetic queries and train/fine-tune a hugging face sentence transformer model on synthetic data<a class="headerlink" href="#Step-4:-Read-synthetic-queries-and-train/fine-tune-a-hugging-face-sentence-transformer-model-on-synthetic-data" title="Link to this heading"></a></h3>
<p>With a synthetic queries zip file, users can fine tune a sentence transformer model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">SentenceTransformerModel</span></code> class will inititate an object for training, exporting and configuring the model. Plese visit the <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.html#opensearch_py_ml.sentence_transformer_model.SentenceTransformerModel">SentenceTransformerModel</a> for API Reference .</p>
<p>The <code class="docutils literal notranslate"><span class="pre">train</span></code> function will import synthestic queries, load sentence transformer example and train the model using a hugging face sentence transformer model. Plese visit the <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.html#opensearch_py_ml.sentence_transformer_model.SentenceTransformerModel.train">SentenceTransformerModel.train</a> for API Reference .</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># clean up cache before training to free up spaces</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="n">training</span> <span class="o">=</span> <span class="n">custom_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">read_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/clean_synthetic_queries.zip&#39;</span><span class="p">,</span>
                        <span class="n">output_model_name</span> <span class="o">=</span> <span class="s1">&#39;test2_model.pt&#39;</span><span class="p">,</span>
                        <span class="n">zip_file_name</span><span class="o">=</span> <span class="s1">&#39;test2_model.zip&#39;</span><span class="p">,</span>
                        <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                        <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                        <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Following are some important points about the training cell executed above,</p>
<ol class="arabic simple">
<li><p>The input to the training script consists of (query, passage) pairs. The model is trained to maximize the dot product between relevant queries and passages while at the same time minimize the dot product between queries and irrelevant passages. This is also known as contrastive learning. We implement this using in-batch negatives and a symmetric loss as mentioned below.</p></li>
<li><p>To utilize the power of GPUs we collect training samples into a batch before sending for model training. Each batch contains B number of randomly selected training samples (q, p). Thus within a batch each query has one relevant passage and B-1 irrelevant passages. Similarly for every passage there’s one relevant query and B-1 irrelevant queries. For every given relevant query and passage pair we minimize the following expression, called the loss,</p></li>
<li><p>For a given batch B, the loss is defined as loss = C(q, p) + C(p, q) where <span class="math notranslate nohighlight">\(C(q, p) = - \sum_{i=1}^{i=B} \log \left( \frac{exp(q_i \cdot p_i)}{\sum_{j=1} ^{B} exp(q_i \cdot p_j)}\right)\)</span></p></li>
<li><p>The model truncates documents beyond 512 tokens. If the corpus contains documents that are shorter than 512 tokens the model max length can be adjusted to that number. Shorter sequences take less memory and therefore allow for bigger batch sizes. The max length can be adjusted by the “percentile” argument.</p></li>
<li><p>We use a batch size of 32 per device. Larger batch sizes lead to more in-batch negative samples and lead to better performance but unfortunately they also lead to out of memory issues. Shorter sequences use less memory, so if the document corpus is short feel free to experiment with larger batch sizes.</p></li>
<li><p>The model is trained using the AdamW optimizer for 10 epochs with a learning rate of 2e-5 and a scheduler with linear schedule with warmup steps = 10,000</p></li>
</ol>
</section>
<section id="Step-5:-Upload-the-model-to-OpenSearch-cluster">
<h3>Step 5: Upload the model to OpenSearch cluster<a class="headerlink" href="#Step-5:-Upload-the-model-to-OpenSearch-cluster" title="Link to this heading"></a></h3>
<p>After generated a model zip file, the users will need to describe model configuration in a ml-commons_model_config.json file. The <code class="docutils literal notranslate"><span class="pre">make_model_config_json</span></code> function in sentencetransformermodel class will parse the config file from hugging-face config.son file. If users would like to use a different config than the pre-trained sentence transformer, <code class="docutils literal notranslate"><span class="pre">make_model_config_json</span></code> function provide arguuments to change the configuration content and generated a ml-commons_model_config.json file. Plese
visit the <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.html#opensearch_py_ml.sentence_transformer_model.SentenceTransformerModel.make_model_config_json">SentenceTransformerModel.make_model_config_json</a> for API Reference .</p>
<p>In general, the ml common client supports uploading sentence transformer models. With a zip file contains model in Torch Script format, and a configuration file for tokenizers in json format, the <code class="docutils literal notranslate"><span class="pre">upload_model</span></code> function connects to opensearch through ml client and upload the model. Plese visit the <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons_integration.MLCommonClient.upload_model">MLCommonClient.upload_model</a>
for API Reference.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#users will need to prepare a ml-commons_model_config.json file to config the model, including model name ..</span>
<span class="c1">#this is a helpful function in py-ml.sentence_transformer_model to generate ml-commons_model_config.json file</span>
<span class="n">custom_model</span><span class="o">.</span><span class="n">make_model_config_json</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#connect to ml_common client with OpenSearch client</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">opensearch_py_ml</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">oml</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">opensearch_py_ml.ml_commons</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLCommonClient</span>
<span class="n">ml_client</span> <span class="o">=</span> <span class="n">MLCommonClient</span><span class="p">(</span><span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># upload model to OpenSearch cluster, using model zip file path and ml-commons_model_config.json file generated above</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/all-MiniLM-L6-v2.zip&#39;</span>
<span class="n">model_config_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/model_config.json&#39;</span>
<span class="n">ml_client</span><span class="o">.</span><span class="n">upload_model</span><span class="p">(</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">model_config_path</span><span class="p">,</span> <span class="n">isVerbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Total number of chunks 10
Sha1 value of the model file:  61fd5a1425960681da49d084dca0e52fd0fabcc0f2e1c4d57c4e20e193bde483
Model meta data was created successfully. Model Id:  lGFG9IUBTo3f8n5R8nM6
uploading chunk 1 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 2 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 3 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 4 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 5 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 6 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 7 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 8 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 9 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 10 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
Model uploaded successfully
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;lGFG9IUBTo3f8n5R8nM6&#39;
</pre></div></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="online_retail_analysis.html" class="btn btn-neutral float-left" title="Demo Notebook for Online Retail Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="demo_tracing_model_torchscript_onnx.html" class="btn btn-neutral float-right" title="Demo Notebook to trace Sentence Transformers model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Opensearch.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>