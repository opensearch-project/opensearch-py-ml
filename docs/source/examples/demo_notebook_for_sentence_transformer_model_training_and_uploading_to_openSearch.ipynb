{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba4c02d",
   "metadata": {},
   "source": [
    "# Demo Notebook for Sentence Transformer Model Training and Uploading to OpenSearch\n",
    "This notebook provides a walkthrough step-by-step guidance for users use their synthetic queries to fine tune and train a sentence transformer model to get a semantic search model. In this notebook, you use opensearch_py_ml to accomplish the following:\n",
    "\n",
    "Step 0: Import packages and Set up Client\n",
    "\n",
    "Step 1: Read synthetic queries and train/fine-tune model using a hugging face sentence transformer model\n",
    "\n",
    "Step 2: Upload the custom model to OpenSearch cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729e8df",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011727e",
   "metadata": {},
   "source": [
    "# Step 0: Import packages and Set up Client\n",
    "Install opensearchpy and opensearch_py_ml through pippy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c021df",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/opensearchpy/connection/http_urllib3.py:208: UserWarning: Connecting to https://instance:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/opensearchpy/connection/http_urllib3.py:208: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/opensearch_py_ml/common.py:359: UserWarning: OpenSearch major version (1.0.0) doesn't match the major version of the OpenSearch server (2.4.0) which can lead to compatibility issues. Your major version should be the same as your cluster major version.\n",
      "  OS_VERSION = os_version(OPENSEARCH_TEST_CLIENT)\n"
     ]
    }
   ],
   "source": [
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.sentence_transformer_model import SentenceTransformerModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798cac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlcommon to later upload the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons_integration import MLCommonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c85ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER_URL = 'https://localhost:9200'\n",
    "CLUSTER_URL = 'https://localhost:9200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77442abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='admin'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url],\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e1cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_os_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9e0de",
   "metadata": {},
   "source": [
    "# Step 1: Import Synthetic Queries and Train/Fine-tune Model\n",
    "With a synthetic queries zip file, users can import opensearch_py_ml package to fine tune a sentence transformer model to train a semantic search model. The train_semantic_search_model function will Import synthestic queries, load sentence transformer example and train the model using a hugging face sentence transformer model. \n",
    "\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    read the synthetic queries and use it to fine tune/train a sentence transformer model to save a zip file\n",
    "\n",
    "    Parameters:\n",
    "    read_path: str\n",
    "        required, path to read the generated queries zip file, if None, default as 'synthetic_query' folder \n",
    "        in current directory\n",
    "    model_id: str = None\n",
    "        optional, the url to download sentence transformer model, if None, default as \n",
    "        'sentence-transformers/msmarco-distilbert-base-tas-b\n",
    "    output_model_path: str=None\n",
    "        optional, the path to store trained custom model. If None, default as current folder path\n",
    "    output_model_name: str=None\n",
    "        optional, the name of the trained custom model. If None, default as 'trained_model.pt'\n",
    "    zip_file_name: str =None\n",
    "        optional, file name for zip file. if None, default as custom_tasb_model.zip\n",
    "    use_accelerate: bool = False,\n",
    "        Optional, use accelerate to fine tune model. Default as false to not use accelerator to fine tune model.\n",
    "        If there are multiple gpus available in the machine, it's recommended to use accelerate with\n",
    "        num_processor>1 to speeed up the training progress. If use accelerator to train model, run auto setup \n",
    "        accelerate confi and launch train_model function with the number of processors provided by users \n",
    "        if NOT use accelerator,trigger train_model function with default setting\n",
    "    compute_environment: str\n",
    "        optional, compute environment type to run model, if None, default using 'LOCAL_MACHINE'\n",
    "    num_machines: int\n",
    "        optional, number of machine to run model , if None, default using 1\n",
    "    num_processes: int\n",
    "        optional, number of processors to run model , if None, default using 1\n",
    "    learning_rate: float\n",
    "        optional, learning rate to train model, default is 2e-5\n",
    "    num_epochs: int\n",
    "        optional, number of epochs to train model, default is 20\n",
    "    verbose: bool\n",
    "        optional, use plotting to plot the training progress. Default as false.\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a337ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up cache before training to free up spaces\n",
    "import gc, torch\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b37e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading synthetic query file: /home/ec2-user/SageMaker/opensearch-py-ml-mingshl/synthetic_queries/output.p\n",
      "Loading training examples... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [00:00<00:00, 781628.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated config file: at/home/ec2-user/.cache/huggingface/accelerate/default_config.yaml\n",
      "[{'compute_environment': 'LOCAL_MACHINE', 'deepspeed_config': {'gradient_accumulation_steps': 1, 'offload_optimizer_device': 'none', 'offload_param_device': 'none', 'zero3_init_flag': False, 'zero_stage': 2}, 'distributed_type': 'DEEPSPEED', 'downcast_bf16': 'no', 'fsdp_config': {}, 'machine_rank': 0, 'main_process_ip': None, 'main_process_port': None, 'main_training_function': 'main', 'mixed_precision': 'no', 'num_machines': 1, 'num_processes': 2, 'use_cpu': False}]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with accelerator...Start training with accelerator...\n",
      "\n",
      "The number of training epoch are 20The number of training epoch are 20\n",
      "\n",
      "The total number of steps training epoch are 25The total number of steps training epoch are 25\n",
      "\n",
      "Training epoch 0...Training epoch 0...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.55it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 22.39643096923828\n",
      "Total training time: 22.410019636154175\n",
      "Preparing model to save\n",
      "Preparing model to save\n",
      "Model saved to path: /home/ec2-user/SageMaker/opensearch-py-ml-mingshl/test_model.pt\n",
      "Model saved to path: /home/ec2-user/SageMaker/opensearch-py-ml-mingshl/test_model.pt\n",
      "zip file is saved to /home/ec2-user/SageMaker/opensearch-py-ml-mingshl/test_model.zip\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformerModel()\n",
    "training = model.train(read_path = '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/synthetic_queries.zip',\n",
    "                        output_model_name = 'test_model.pt',\n",
    "                        zip_file_name= 'test_model.zip',\n",
    "                        overwrite = True,\n",
    "                        use_accelerate  = True,  \n",
    "                        num_machines = 1,\n",
    "                        num_processes = 2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd0405",
   "metadata": {},
   "source": [
    "# Step 2: Upload the model to OpenSearch cluster\n",
    "connect to opensearch through client and upload the model zip file output from step 1.\n",
    "\n",
    "In general, the ml common client supports uploading Machine learning models, using upload_model function. The upload_model function takes three agruments:\n",
    "\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    upload a zip file and model_config file to OpenSearch cluster. \n",
    "    Parameters:\n",
    "    model_path: str\n",
    "        file path of the model file (zip file expected). The zip file should contain two files. The first file \n",
    "        is a model file in Torch Script format, e.g, \"model.pt\". The second file is a configuration file for\n",
    "        tokenizers in json format, e.g, \"tokenizers.json\".\n",
    "    model_config_file: str\n",
    "        file path of the model config file (json file expected), which includes necessary config info for the \n",
    "        model, including model name, version number and etc. The details for model configuration are here: \n",
    "        https://opensearch.org/docs/latest/ml-commons-plugin/model-serving-framework/. An example file will \n",
    "        show in the below cells. \n",
    "    Return: \n",
    "        None\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe84425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to ml_common client with OpenSearch client\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "260b6b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'all-MiniLM-L6-v2', 'version': 1, 'model_format': 'TORCH_SCRIPT', 'model_task_type': 'TEXT_EMBEDDING', 'model_config': {'model_type': 'bert', 'embedding_dimension': 384, 'framework_type': 'sentence_transformers', 'all_config': '{\"_name_or_path\":\"nreimers/MiniLM-L6-H384-uncased\",\"architectures\":[\"BertModel\"],\"attention_probs_dropout_prob\":0.1,\"gradient_checkpointing\":false,\"hidden_act\":\"gelu\",\"hidden_dropout_prob\":0.1,\"hidden_size\":384,\"initializer_range\":0.02,\"intermediate_size\":1536,\"layer_norm_eps\":1e-12,\"max_position_embeddings\":512,\"model_type\":\"bert\",\"num_attention_heads\":12,\"num_hidden_layers\":6,\"pad_token_id\":0,\"position_embedding_type\":\"absolute\",\"transformers_version\":\"4.8.2\",\"type_vocab_size\":2,\"use_cache\":true,\"vocab_size\":30522}'}}\n"
     ]
    }
   ],
   "source": [
    "#user will need to prepare a model_config.json file to config the model, including model name .. \n",
    "#this is a sample of model_config.json file\n",
    "  \n",
    "f = open('model_config.json')\n",
    "  \n",
    "model_config = json.load(f)\n",
    "  \n",
    "print(model_config)\n",
    "f.close=()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d1589",
   "metadata": {},
   "source": [
    "### If using the model zip generated from training script in step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b0ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 27\n",
      "Sha1 value of the model file:  d1fc88bc317ed3dc52c4c7dc4d51122c6989876e62167566289a8067ab5d51e7\n",
      "Model meta data was created successfully. Model Id:  oglVm4QBDmk7AZE7yW-d\n",
      "uploading chunk 1 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 2 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 3 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 4 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 5 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 6 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 7 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 8 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 9 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 10 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 11 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 12 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 13 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 14 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 15 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 16 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 17 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 18 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 19 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 20 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 21 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 22 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 23 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 24 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 25 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 26 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 27 of 27\n",
      "{'status': 'Uploaded'}\n",
      "Model uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "# upload model to OpenSearch cluster, using model zip file from step 1\n",
    "\n",
    "model_path = '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/test_model.zip'\n",
    "ml_client.upload_model( model_path, '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/save_pre_trained_model_json/model_config.json', isVerbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d79802",
   "metadata": {},
   "source": [
    "### If Using Other Pretrained Sentence Transformer model from Hugging face\n",
    "\n",
    "Users can use save_as_pt function to experiement a pre-trained sentence transformer model for inferencing or benchmark with other models. \n",
    "\n",
    "The save_as_pt function will prepare the model in proper format along with configuration file to upload to OpenSearch. The save_as_pt function takes the following arguments. \n",
    "\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        download sentence transformer model directly from huggingface, convert model to torch script format,\n",
    "        zip the model file and its tokenizer.json file to prepare to upload to the Open Search cluster\n",
    "\n",
    "        Parameters:\n",
    "        sentences:[str]\n",
    "            Required, for example  sentences = ['today is sunny']\n",
    "        model: str\n",
    "            Optional, if provide model in parameters, will convert model to torch script format,\n",
    "            else, not provided model then it will download sentence transformer model from huggingface. \n",
    "            If None, default takes model_id = \"sentence-transformers/msmarco-distilbert-base-tas-b\". \n",
    "        model_name: str\n",
    "            Optional, model name to name the model file, e.g, \"sample_model.pt\". If None, default takes the \n",
    "            model_id and add the extension with \".pt\".\n",
    "        zip_file_name: str =None\n",
    "            Optional, file name for zip file. e.g, \"sample_model.zip\". If None, default takes the model_id \n",
    "            and add the extension with \".zip\".\n",
    "            None\n",
    "\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "447a9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file is saved to/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/msmarco-distilbert-base-tas-b.pt\n",
      "zip file is saved to /home/ec2-user/SageMaker/opensearch-py-ml-mingshl/msmarco-distilbert-base-tas-b.zip\n"
     ]
    }
   ],
   "source": [
    "# default to download model id, \"sentence-transformers/msmarco-distilbert-base-tas-b\" from hugging face \n",
    "# and output a model in a zip file containing model.pt file and tokenizers.json file. \n",
    "model = SentenceTransformerModel()\n",
    "model.save_as_pt(sentences = ['today is sunny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/msmarco-distilbert-base-tas-b.zip'\n",
    "ml_client.upload_model( model_path, '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/save_pre_trained_model_json/model_config.json', isVerbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8abc9eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b604f79acc4fef8de19c172ead6707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5b0dca6462405aa7edc76ff5bd4744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2ed1c2ca74496b9a595e769d89eee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23e99e506694894b773152f400cd4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d50e23bbc54894955837cbcdebabc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdce20ddf724e89a9cbd65c59b5b26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/7.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28089cdfea8345a88b635d2362ce9657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f95073f458e422280a92228eb230e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfd5e2f13484d889fa91988940db212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31523a8d49f04c9ca9f5e427b39d59e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fbdf97827b41f9ab15afccbb542adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018a283cb7d44f3fbe994f5a5ce0b1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3e78620cea49c7b4c23c8a25fe51bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086c2dfd2e8140d38045f69ec98cfdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a7f431077641d7a17ffddd65032728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fdd39b061749bfb9d9af3b0931ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e94f62b49f41b1a6855d201645b3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file is saved to/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/msmarco-roberta-base-ance-firstp.pt\n",
      "zip file is saved to /home/ec2-user/SageMaker/opensearch-py-ml-mingshl/msmarco-roberta-base-ance-firstp.zip\n"
     ]
    }
   ],
   "source": [
    "# download another model id from hugging face and convert as zip file,\n",
    "# using example \"sentence-transformers/msmarco-roberta-base-ance-firstp\" as following:\n",
    "model = SentenceTransformerModel(model_id = 'sentence-transformers/msmarco-roberta-base-ance-firstp')\n",
    "model.save_as_pt(sentences = ['today is sunny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cde52d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 27\n",
      "Sha1 value of the model file:  bfe59d138525994438bd240b29dc8807a7257529755d706ef240ba587f8f4006\n",
      "Model meta data was created successfully. Model Id:  owmzm4QBDmk7AZE7bm-i\n",
      "uploading chunk 1 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 2 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 3 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 4 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 5 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 6 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 7 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 8 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 9 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 10 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 11 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 12 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 13 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 14 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 15 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 16 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 17 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 18 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 19 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 20 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 21 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 22 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 23 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 24 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 25 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 26 of 27\n",
      "{'status': 'Uploaded'}\n",
      "uploading chunk 27 of 27\n",
      "{'status': 'Uploaded'}\n",
      "Model uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "# upload \"msmarco-distilbert-base-tas-b\" model to OpenSearch cluster \n",
    "model_path = '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/msmarco-distilbert-base-tas-b.zip'\n",
    "ml_client.upload_model( model_path, '/home/ec2-user/SageMaker/opensearch-py-ml-mingshl/save_pre_trained_model_json/model_config.json', isVerbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
