{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba4c02d",
   "metadata": {},
   "source": [
    "# Demo Notebook to trace Sentence Transformers model\n",
    "\n",
    "This notebook provides a walkthrough guidance for users to trace models from Sentence Transformers in torchScript and onnx format. After tracing the model customer can upload the model to opensearch and generate embeddings.\n",
    "\n",
    "Remember, tracing model in torchScript or Onnx format at just two different options. We don't need to trace model in both ways. Here in our notebook we just want to show both ways. \n",
    "\n",
    "Step 0: Import packages and set up client\n",
    "\n",
    "Step 1: Save model in torchScript format\n",
    "\n",
    "Step 2: Upload the saved torchScript model in Opensearch\n",
    "\n",
    "[The following steps are optional, just showing uploading model in both ways and comparing the both embedding output]\n",
    "\n",
    "Step 3: Save model in Onnx format \n",
    "\n",
    "Step 4: Upload the saved Onnx model in Opensearch\n",
    "\n",
    "Step 5: Generate Sentence Embedding with uploaded models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011727e",
   "metadata": {},
   "source": [
    "## Step 0: Import packages and set up client\n",
    "Install required packages for opensearch_py_ml.sentence_transformer_model\n",
    "Install `opensearchpy` and `opensearch-py-ml` through pypi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a3e085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install opensearch-py opensearch-py-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c711bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "# import mlcommon to later upload the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c85ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_URL = 'https://localhost:9200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77442abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='admin'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url],\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e1cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_os_client()\n",
    "\n",
    "#connect to ml_common client with OpenSearch client\n",
    "import opensearch_py_ml as oml\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9e0de",
   "metadata": {},
   "source": [
    "## Step 1: Save model in torchScript format\n",
    "\n",
    "`Opensearch-py-ml` plugin provides method `save_as_pt` which will trace a model in torchScript format and save the model in a zip file in your filesystem. \n",
    "\n",
    "Detailed documentation: https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_pt.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_pt\n",
    "\n",
    "\n",
    "Users need to provide a model id from sentence transformers (an example: `sentence-transformers/all-MiniLM-L6-v2`). This model id is a huggingface model id. Exaample: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "`save_as_pt` will download the model in filesystem and then trace the model with the given input strings.\n",
    "\n",
    "To get more direction about dummy input string please check this url: https://huggingface.co/docs/transformers/torchscript#dummy-inputs-and-standard-lengths\n",
    "\n",
    "after tracing the model (a .pt file will be generated), `save_as_pt` method zips `tokenizers.json` and torchScript (`.pt`) file and saves in the file system. \n",
    "\n",
    "User can upload that model to opensearch to generate embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b0ff7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file is saved to  /Volumes/workplace/upload_content/all-MiniLM-L6-v2.pt\n",
      "zip file is saved to  /Volumes/workplace/upload_content/all-MiniLM-L6-v2.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = SentenceTransformerModel(folder_path = '/Volumes/workplace/upload_content/', overwrite = True)\n",
    "model_path = pre_trained_model.save_as_pt(model_id = \"sentence-transformers/all-MiniLM-L6-v2\", sentences=[\"for example providing a small sentence\", \"we can add multiple sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b7cbd",
   "metadata": {},
   "source": [
    "## Step 2: Upload the saved torchScript model in Opensearch\n",
    "\n",
    "In the last step we saved a sentence transformer model in torchScript format. Now we will upload that model in opensearch cluster. To do that we can take help of `upload_model` method in `opensearch-py-ml` plugin.\n",
    "\n",
    "To upload model, we need the zip file we just saved in the last step and a model config file. Example of Model config file content can be:\n",
    "\n",
    "{\n",
    "  \"name\": \"all-MiniLM-L6-v2\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"description\": \"test model\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": 384,\n",
    "    \"framework_type\": \"sentence_transformers\"\n",
    "  }\n",
    "}\n",
    "\n",
    "`model_format` needs to be `TORCH_SCRIPT` so that internal system will look for the corresponding `.pt` file from the zip folder. \n",
    "\n",
    "Please refer to this doc: https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md\n",
    "\n",
    "\n",
    "Documentation for the method: https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model\n",
    "\n",
    "Related demo notebook about ml-commons plugin integration: https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e9310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 10\n",
      "Sha1 value of the model file:  0a8eabed8c09b09b588e9d4c3d42e61c90e524d014be5547b73db75e77185576\n",
      "Model meta data was created successfully. Model Id:  SGHQ9IUBTo3f8n5RC3ZH\n",
      "uploading chunk 1 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 10 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model uploaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SGHQ9IUBTo3f8n5RC3ZH'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_config_path = '/Volumes/workplace/upload_content/model_config_torchscript.json'\n",
    "ml_client.upload_model( model_path, model_config_path, isVerbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee235b",
   "metadata": {},
   "source": [
    "## Step 3:Save model in Onnx format\n",
    "\n",
    "`Opensearch-py-ml` plugin provides method `save_as_onnx` which will trace a model in ONNX format and save the model in a zip file in your filesystem. \n",
    "\n",
    "Detailed documentation: https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_onnx.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_onnx\n",
    "\n",
    "\n",
    "Users need to provide a model id from sentence transformers (an example: `sentence-transformers/all-MiniLM-L6-v2`). `save_as_onnx` will download the model in filesystem and then trace the model.\n",
    "\n",
    "after tracing the model (a .onnx file will be generated), `save_as_onnx` method zips `tokenizers.json` and torchScript (`.onnx`) file and saves in the file system. \n",
    "\n",
    "User can upload that model to opensearch to generate embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d6b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 16\n",
      "Loading pipeline (model: sentence-transformers/all-MiniLM-L6-v2, tokenizer: sentence-transformers/all-MiniLM-L6-v2)\n",
      "Creating folder /Volumes/workplace/upload_content/onnx\n",
      "Using framework PyTorch: 1.13.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n",
      "zip file is saved to  /Volumes/workplace/upload_content/all-MiniLM-L6-v2.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = SentenceTransformerModel(folder_path = '/Volumes/workplace/upload_content/', overwrite = True)\n",
    "model_path_onnx = pre_trained_model.save_as_onnx(model_id = \"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed5665",
   "metadata": {},
   "source": [
    "## Step 4: Upload the saved Onnx model in Opensearch\n",
    "\n",
    "In the last step we saved a sentence transformer model in ONNX format. Now we will upload that model in opensearch cluster. To do that we can take help of `upload_model` method in `opensearch-py-ml` plugin.\n",
    "\n",
    "To upload model, we need the zip file we just saved in the last step and a model config file. Example of Model config file content can be:\n",
    "\n",
    "{\n",
    "  \"name\": \"all-MiniLM-L6-v2\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"description\": \"test model\",\n",
    "  \"model_format\": \"ONNX\",\n",
    "  \"model_config\": {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"embedding_dimension\": 384,\n",
    "    \"framework_type\": \"sentence_transformers\",\n",
    "    \"pooling_mode\":\"mean\",\n",
    "    \"normalize_result\":\"true\"\n",
    "  }\n",
    "}\n",
    "\n",
    "`model_format` needs to be `ONNX` so that internal system will look for the corresponding `.onnx` file from the zip folder.\n",
    "\n",
    "Please refer to this doc: https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md\n",
    "\n",
    "\n",
    "Documentation for the method: https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model\n",
    "\n",
    "Related demo notebook about ml-commons plugin integration: https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661c3f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 10\n",
      "Sha1 value of the model file:  8faa075a1693f3734f956a8c8e0de755ad4142b59df6d605298879b0dab31308\n",
      "Model meta data was created successfully. Model Id:  YWHS9IUBTo3f8n5ReXas\n",
      "uploading chunk 1 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 10 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model uploaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'YWHS9IUBTo3f8n5ReXas'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_config_path = '/Volumes/workplace/upload_content/model_config.json'\n",
    "ml_client.upload_model( model_path_onnx, model_config_path, isVerbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22c708",
   "metadata": {},
   "source": [
    "## Step 5: Generate Sentence Embedding\n",
    "\n",
    "Now after loading these models in memory, we can generate embedding for sentences. We can provide a list of sentences to get a list of embedding for the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc5a796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now using this model we can generate sentence embedding.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_sentences = [\"first sentence\", \"second sentence\"]\n",
    "\n",
    "# generated embedding from torchScript\n",
    "\n",
    "embedding_output_torch = ml_client.generate_embedding(\"SGHQ9IUBTo3f8n5RC3ZH\", input_sentences)\n",
    "\n",
    "#just taking embedding for the first sentence\n",
    "data_torch = embedding_output_torch[\"inference_results\"][0][\"output\"][0][\"data\"]\n",
    "\n",
    "# generated embedding from onnx\n",
    "\n",
    "embedding_output_onnx = ml_client.generate_embedding(\"YWHS9IUBTo3f8n5ReXas\", input_sentences)\n",
    "\n",
    "#just taking embedding for the first sentence\n",
    "data_onnx = embedding_output_onnx[\"inference_results\"][0][\"output\"][0][\"data\"]\n",
    "\n",
    "## Now we can check if there's any significant difference between two outputs\n",
    "\n",
    "print(np.testing.assert_allclose(data_torch, data_onnx, rtol=1e-03, atol=1e-05))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}